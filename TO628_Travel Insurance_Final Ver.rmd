---
title: "TO628_Travel Insurance"
author: "Helga Zhang, Caio Rodrigues, Yuwei Liu & Yehudah Gol"
date: "2024-04-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

> Story-telling Background

## *Storytelling* ##

Industry - Travel Insurance
Audience - Customer Engagement Manager:
Improve profitability of their operation

Big Idea:
Identify characteristics of target customer;
Help with targeted marketing efforts

Quantitative measurement factors:
Increase in return on marketing spend 
Increase in revenue

## **Business Questions** ##

Why our prediction is relevant to your business?
How our prediction help your business minimum costs and maximum profits?
How does your business imply the output of our prediction?

## **Answering Business Questions** ##

1. Provide you what type of people are more likely to purchase insurance:
2. Higher percentage of making successful call
3. Better resource allocation decision, reduce costs
4. Great fit with company’s own data:
5. Customize with the factor that is unique to you business
6. Block noise and distractions in the dataset: Solely focus on the useful information
7. Develop insights for customer behaviors

> Load Data & Clean Data

```{r}
travel <- read.csv("Travel Insurance Prediction.csv")
str(travel)
summary(travel)

#factorize chr
travel$EmploymentType <- as.factor(travel$EmploymentType)
travel$GraduateOrNot <- as.factor(travel$GraduateOrNot)
travel$FrequentFlyer <- as.factor(travel$FrequentFlyer)
travel$EverTravelledAbroad <- as.factor(travel$EverTravelledAbroad)
travel$ChronicDiseases <- as.factor(travel$ChronicDiseases) # Assuming chronic diseases is also a binary factor
str(travel)

library(readr)
library(dplyr)
library(caret)
library(neuralnet) 
library(e1071) 
library(randomForest)
library(class)
library(rpart)
library(C50)
```

> Split Test and Train Data

```{r}
# Split the data
set.seed(123) # Ensure reproducibility
splitIndex <- createDataPartition(travel$TravelInsurance, p = .5, list = FALSE, times = 1)
train_set <- travel[splitIndex,]
test_set <- travel[-splitIndex,]

# Preparing for data modeling
train_x <- train_set %>% select(-TravelInsurance)
train_y <- train_set$TravelInsurance
test_x <- test_set %>% select(-TravelInsurance)
test_y <- test_set$TravelInsurance
# Convert the response variable in both training and testing sets into a factor with explicit levels
train_y <- factor(train_set$TravelInsurance, levels = c(0, 1))
test_y <- factor(test_set$TravelInsurance, levels = c(0, 1))
```

> Build Models

## LR Model

```{r}
# Build the model
logit_model <- glm(TravelInsurance ~ ., data = train_set, family = "binomial")

# Predict on the test set
logit_pred <- predict(logit_model, newdata = test_set, type = "response")
logit_pred_class <- ifelse(logit_pred > 0.5, 1, 0)

# Confusion Matrix and Kappa
cm_logit <- confusionMatrix(factor(logit_pred_class), factor(test_y))
print(cm_logit)

# Summary of the model to examine factors
summary(logit_model)

logit_model_big <- glm(TravelInsurance ~ . + .*. , data = train_set, family = "binomial")

LR_step <- step(logit_model_big, direction = "backward")

summary(logit_model_big)
summary(LR_step)

# Predict on the test set
logit_big_pred <- predict(logit_model_big, newdata = test_set, type = "response")
logit_big_pred_class <- ifelse(logit_big_pred > 0.5, 1, 0)

LR_step_pred <- predict(LR_step, newdata = test_set, type = "response")
LR_step_pred_class <- ifelse(LR_step_pred > 0.5, 1, 0)

# Confusion Matrix and Kappa
cm_logit_big <- confusionMatrix(factor(logit_big_pred_class), factor(test_y))
print(cm_logit_big)

cm_LR_step <- confusionMatrix(factor(LR_step_pred_class), factor(test_y))
print(cm_LR_step)


```

## KNN Model

```{r}
# Normalize data function
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# Normalize training and testing sets (excluding factors)
numeric_columns <- sapply(train_x, is.numeric)
train_x_normalized <- as.data.frame(sapply(train_x[, numeric_columns], normalize))
test_x_normalized <- as.data.frame(sapply(test_x[, numeric_columns], normalize))

# Add back the factor columns
train_x_normalized <- cbind(train_x_normalized, train_x[!numeric_columns])
test_x_normalized <- cbind(test_x_normalized, test_x[!numeric_columns])

# Convert factors to dummy variables for KNN
train_x_dummy <- dummyVars(" ~ .", data = train_x_normalized)
train_x_prepared <- predict(train_x_dummy, newdata = train_x_normalized)

test_x_dummy <- dummyVars(" ~ .", data = test_x_normalized)
test_x_prepared <- predict(test_x_dummy, newdata = test_x_normalized)

# KNN model
set.seed(123)
knn_pred <- knn(train = train_x_prepared, test = test_x_prepared, cl = train_y, k = 5)
# Ensuring 'test_y' is a factor with correct levels
test_y <- factor(test_y, levels = unique(test_y))

# Also ensure 'knn_pred' is a factor and has the same levels as 'test_y'
knn_pred <- factor(knn_pred, levels = levels(test_y))

# Confusion Matrix and Kappa for KNN
cm_knn <- confusionMatrix(knn_pred, test_y)
print(cm_knn)

```

## ANN Model

```{r}
#### ANN Model
set.seed(123)

#prepare data
train_ann<-as.data.frame(cbind(train_x_prepared,train_y))
colnames(train_ann) <- make.names(colnames(train_ann), unique=TRUE)
train_ann$train_y<-ifelse(train_ann$train_y==1,0,1)

test_ann<-as.data.frame(cbind(test_x_prepared,test_y))
colnames(test_ann) <- make.names(colnames(test_ann), unique=TRUE)
test_ann$test_y<-ifelse(test_ann$test_y==1,1,0)

#### ANN Model
ann_model <- neuralnet(train_y ~ ., data = train_ann, hidden = 5)


# Predict on the test set using the ANN model
ann_pred_prob <- predict(ann_model, newdata = test_ann, type = "raw")

# Convert probabilities to class predictions based on a threshold (e.g., 0.5)
ann_pred_class <- ifelse(ann_pred_prob > 0.5, 1, 0)

# Ensure test_y is a factor with explicit levels
test_y_factor <- factor(test_ann$test_y, levels = unique(test_ann$test_y))

# Ensure ann_pred_class is a factor and has the same levels as test_y_factor
ann_pred_factor <- factor(ann_pred_class, levels = levels(test_y_factor))

# Confusion Matrix and Kappa
cm_ann <- confusionMatrix(ann_pred_factor, test_y_factor)
print(cm_ann)
```

## SVM Model

```{r}
##### SVM Model
# Preparing the data
train_set$TravelInsurance <- factor(train_set$TravelInsurance, levels = c(0, 1))
test_set$TravelInsurance <- factor(test_set$TravelInsurance, levels = c(0, 1))

# Model
svm_model <- svm(TravelInsurance ~ ., data = train_set, kernel = "radial")

# Predict on the test set
svm_pred <- predict(svm_model, newdata = test_set)

# Convert predictions to factor 
svm_pred_factor <- factor(svm_pred, levels = levels(test_y))

# Confusion Matrix and Kappa 
cm_svm <- confusionMatrix(svm_pred_factor, test_y)
print(cm_svm)
```

## DT Model

```{r}
#### Decision Tree Model

dt_model <- C5.0(as.factor(TravelInsurance) ~ ., data = train_set)

# Predict on the test set 
dt_pred <- predict(dt_model, newdata = test_set, type = "class")

# Convert predictions to factor 
dt_pred_factor <- factor(dt_pred, levels = levels(test_y))

# Confusion Matrix and Kappa 
cm_dt <- confusionMatrix(dt_pred_factor, test_y)
print(cm_dt)

```

## RF Model

```{r}
##### Random Forest Model

# Getting data ready
train_set$TravelInsurance <- factor(train_set$TravelInsurance, levels = c(0, 1))
test_set$TravelInsurance <- factor(test_set$TravelInsurance, levels = c(0, 1))

# Model
rf_model <- randomForest(TravelInsurance ~ ., data = train_set, ntree = 100)

# Predictions
rf_pred <- predict(rf_model, newdata = test_set)
rf_pred_factor <- factor(rf_pred, levels = levels(test_y))

# Confusion Matrix and Kappa
cm_rf <- confusionMatrix(rf_pred_factor, test_y)
print(cm_rf)

```

## Stacked Model

```{r}
#### Starting the Stacked Model
# Combine prediction vectors 

combined_predictions <- data.frame(
  LogisticRegression = logit_pred_class,
  KNN = knn_pred,
  ANN = ann_pred_class,
  SVM = svm_pred_factor,
  DT = dt_pred_factor,
  RF = rf_pred_factor,
  Actual = test_y
)

# Structure of the new data frame
str(combined_predictions)

# Assuming a simple 50/50 split 
set.seed(123)
splitIndex2 <- createDataPartition(combined_predictions$Actual, p = .5, list = FALSE)
combined_train <- combined_predictions[splitIndex2, ]
combined_test <- combined_predictions[-splitIndex2, ]

# Build the model
stacked_dt_model <- C5.0(Actual ~ ., data = combined_train, method = "class")

# Summary of the model
summary(stacked_dt_model)

# Predict on the combined test set
stacked_dt_pred <- predict(stacked_dt_model, newdata = combined_test, type = "class")

# Convert predictions to factor 
stacked_dt_pred_factor <- factor(stacked_dt_pred, levels = levels(combined_test$Actual))

# confusion matrix and Kappa
cm_stacked_dt <- confusionMatrix(stacked_dt_pred_factor, combined_test$Actual)
print(cm_stacked_dt)

```

## Cost Model

```{r}
cost_matrix <- matrix(c(0,5,3,0), nrow = 2)
cost_matrix
cost_model <- stacked_dt_model <- C5.0(as.factor(Actual) ~ . , data = combined_train, costs = cost_matrix)

pred_cost <- predict(cost_model, combined_test)

cm_costmodel<-confusionMatrix(as.factor(pred_cost), as.factor(combined_test$Actual), positive = "1")
print(cm_costmodel)
```

> Calculate Profits

## Profit ##

```{r}
#Function to calculate profit (Note: 3 Different functions as structure of Confusion Matrix is not fully consistent across models)

profit_calc1<-function(cm){
  TN<-cm$table[2,2]
  TP<-cm$table[1,1]
  FN<-cm$table[2,1]
  FP<-cm$table[1,2]
  profit<-0*TN+5*TP-3*FP+0*FN
  kappa<-cm$overall[2]
  accuracy<-cm$overall[1]
  val<-c(profit,kappa,accuracy)
  return(val)
}

profit_calc2<-function(cm){
  TN<-cm$table[1,1]
  TP<-cm$table[2,2]
  FN<-cm$table[1,2]
  FP<-cm$table[2,1]
  profit<-0*TN+5*TP-3*FP+0*FN
  kappa<-cm$overall[2]
  accuracy<-cm$overall[1]
  val<-c(profit,kappa,accuracy)
  return(val)
}

# Because the train_data used in the stacked model has only 1/2 of the total training data entries, we double the profit here to normalize the number and better match with other models 
profit_calc_stacked<-function(cm){
  TN<-cm$table[2,2]
  TP<-cm$table[1,1]
  FN<-cm$table[2,1]
  FP<-cm$table[1,2]
  profit<-2*(0*TN+5*TP-3*FP+0*FN)
  kappa<-cm$overall[2]
  accuracy<-cm$overall[1]
  val<-c(profit,kappa,accuracy)
  return(val)
}

#extract Profit and Kappa for each model
DT<-profit_calc1(cm_dt)
KNN<-profit_calc1(cm_knn)
ANN<-profit_calc1(cm_ann)
LR<-profit_calc2(cm_logit)
LR_Big<-profit_calc2(cm_LR_step)
RF<-profit_calc1(cm_rf)
Stacked_Model<-profit_calc_stacked(cm_stacked_dt)
SVM<-profit_calc1(cm_svm)
NoModel<-c(sum(test_set$TravelInsurance==1, na.rm=TRUE)*5-3*nrow(test_set),1,sum(test_set$TravelInsurance==1, na.rm=TRUE)/nrow(test_set))

#create dataframe with all values
summary_df<-data.frame(rbind(DT,KNN,ANN,LR,LR_Big,RF,Stacked_Model,SVM,NoModel))
colnames(summary_df)<-c("Profit", "Kappa","Accuracy")

summary_df


```

> Conclusion

## Summary ##

To sum up, based on Profit, Kappa, and Accuracy, the best performing model is the Random Forest Model. It leads to the highest generated profit, and has the highest Kappa, and accuracy. The stacked model produces $1,974 profits and high Kappa and accuracy from the confusion matrix, showing significant improvements from level 2 model. The Decision Tree model performs very similarly to these two models.

All of the models are better than the scenario with no model, which results in a loss of $2,311. In conclusion, it would make most sense to deploy the random forest model, as it leads to the highest profit, and also performs best in terms of accuracy and Kappa.

## Risk and Limitations ##

The “travel” dataset contains only 3,974 observations in total. This size limitation is further exacerbated in the stacked model, which relies on predictions from the base models, and thus is the “combined_train” dataset used to train the stacked model is only one-fourth as large as the original dataset. Consequently, the small data size may result in a lack of representativeness, potentially introducing biases or overfitting issues that could compromise predictive accuracy when applied to new datasets. Moreover, while the Random Forest model is determined as the best-performing model, to optimize it when applied to new data requires, additional effort of tuning the data- involving the defining parameter grid and perform grid search- could be needed.

The model has only 8 independent variables, which cannot explain all the variations in y variable, so adding more relevant variables- such as travel history and past insurance claims- to the training model would be desirable if the model is to be applied in real business settings. Also the model does not account for changes in these variables over time or the macroeconomic factors like COVID that could potentially affect people’s decisions to purchase travel insurances.
